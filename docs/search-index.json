{
  "documents": [
    {
      "id": "reference/focusgroup-evaluation-2026-01",
      "title": "Focusgroup Agent UX Evaluation (2026-01)",
      "tags": "focusgroup agent-ux evaluation cli",
      "content": "Focusgroup Agent UX Evaluation (2026-01) Comprehensive evaluation of mx CLI agent usability using focusgroup - 5 sessions with 12 total agents (8 Claude, 4 GPT). Overall Rating: 8-8.5/10 Sessions Session ID Focus Agents Key Finding 20260111-cb7d8f20 General usability 4 Claude Validation crashes, JSON error gaps 20260111-d86b9983 General usability 4 Claude 8.5/10 rating 20260111-2b993462 Error messages 2 Claude 80% self-service, no command suggestions 20260111-7c39a291 Intuition test 4 Claude 100% command convergence 20260111-6e6f778b Intuition test 4 GPT More variance, same flag mistakes Key Strengths Feature Why It Works Token-conscious output --compact , --terse , --json serve different context budgets Schema introspection mx schema - &quot;agent gold&quot; for self-discovery Batch operations mx batch reduces round-trips Dry-run support mx add --dry-run allows validation Error handling --json-errors with codes like ENTRY_NOT_FOUND (1002) Issues Found P0: Crashes mx search &quot;test&quot; --limit 0 - Python traceback mx list --category=nonexistent - Python traceback --json-errors doesn't catch Click validation failures P1: Flag Discoverability All 8 agents guessed --find / --replace for patch (actual: --old / --new ) Cross-model consensus = strong signal flags are unintuitive P2: Command Suggestions Flag typos get suggestions: --jason → &quot;Possible options: --json&quot; Command typos don't: mx serach → no &quot;Did you mean 'search'?&quot; Agent-Recommended Patterns # Session start mx prime # Precise search (avoid semantic surprises) mx search &quot;query&quot; --strict --terse --limit=1 # Check existence mx get path.md --metadata # Exit code 0/1 # Batch for efficiency echo -e &quot;search 'docker'\\nget path/file.md&quot; | mx batch # Safe writes mx add --title=&quot;...&quot; --dry-run --json Cross-Model Comparison Metric Claude GPT Response variance 0% (identical) High (3+ variants) Correct commands 8/8 8/8 Correct flags ~6/8 ~5/8 Both models guessed wrong on patch flags - validates this is a real UX gap. Related Beads Issues voidlabs-kb-ktfu - Epic: mx CLI Agent UX Improvements voidlabs-kb-i24u - Patch flag rename (--old/--new → --find/--replace) voidlabs-kb-77q4 - Flag discoverability voidlabs-kb-skeb - Error message improvements voidlabs-kb-pgml - General agent usability voidlabs-kb-jll6 - Append workflow confusion Session Logs Retrieve full session data: focusgroup logs show 20260111-cb7d8f20 focusgroup logs show 20260111-2b993462 focusgroup logs show 20260111-7c39a291 focusgroup logs show 20260111-6e6f778b"
    },
    {
      "id": "reference/entry-format",
      "title": "Entry Format Reference",
      "tags": "reference frontmatter markdown links",
      "content": "Entry Format Reference Memex entries are Markdown files with YAML frontmatter. Basic Structure --- title: Entry Title tags: [tag1, tag2, tag3] created: 2025-01-15 description: One-line summary for search results --- # Entry Title Your content here with [[bidirectional links]] to other entries. Frontmatter Fields Required Field Type Description title string Entry title (used for display and title matching) tags list Tags for filtering and discovery Optional Field Type Description created date Creation date (YYYY-MM-DD) updated date Last update date description string One-line summary for search results aliases list Alternative titles for title matching draft boolean Exclude from mx publish unless --include-drafts archived boolean Exclude from search and publish source_project string Project that created this entry semantic_links list Semantic links (auto-managed unless set via CLI) relations list Typed relations to other entries Semantic Links (optional) Semantic links are usually computed by Memex when semantic linking is enabled. They can also be set explicitly via CLI ( mx add --semantic-links or mx replace --semantic-links ). semantic_links: - path: reference/cli.md score: 0.82 reason: embedding_similarity - path: guides/quick-start.md score: 0.74 reason: shared_tags Typed Relations (optional) Typed relations are manual, directed links that carry a relation type. Add them by editing frontmatter directly (or via mx patch / mx replace ). relations: - path: reference/cli.md type: documents - path: guides/installation.md type: depends_on Canonical relation types Use snake_case and prefer these canonical types. If none fit, use related as the fallback. Type Meaning depends_on A requires B to function or make progress implements A implements the spec, API, or plan described by B extends A builds on or extends B with additional behavior documents A documents or explains B (guides, readmes, specs) references A cites B for supporting detail or context blocks A blocks B from proceeding (directional) related A is generally related to B when no stronger type fits Use mx relations-lint to audit unknown or inconsistent relation types. Bidirectional Links Link to other entries using wiki-style syntax: See [[guides/installation]] for setup instructions. Or use display text: [[guides/installation|the installation guide]]. Link Resolution Links resolve in this order: Exact path match (with or without .md ) Title match (case-insensitive) Filename match (for short links) Examples [[guides/installation]] # Path link [[Installation Guide]] # Title link [[installation]] # Filename link [[guides/installation|Setup]] # With display text Tags Tags enable filtering and discovery: tags: [infrastructure, deployment, docker] Best Practices Use lowercase, hyphenated tags Check existing tags with mx tags Be consistent across entries Use 2-5 tags per entry Common Tag Patterns Pattern Example Technology docker , kubernetes , python Category troubleshooting , patterns , reference Project myapp , api-gateway Status draft , needs-review Content Guidelines Headings Use heading hierarchy for structure: # Main Title (matches frontmatter title) ## Section ### Subsection Code Blocks Use fenced code blocks with language hints: \\`\\`\\`bash mx search &quot;query&quot; \\`\\`\\` \\`\\`\\`python def example(): pass \\`\\`\\` Lists Use consistent list formatting: - Item one - Item two - Nested item 1. First step 2. Second step File Organization Categories Organize entries into directories: kb/ ├── guides/ # How-to guides ├── reference/ # Reference documentation ├── patterns/ # Reusable patterns ├── troubleshooting/ # Problem-solution pairs ├── projects/ # Project-specific docs └── infrastructure/ # Infrastructure docs Naming Use lowercase with hyphens: my-entry-title.md Keep names descriptive but concise Avoid special characters Templates Use templates for consistent structure: # List available templates mx templates # Show a template and copy it into a new entry mx templates show troubleshooting Built-in Templates Template Use case troubleshooting Problem/solution documentation pattern Reusable code/design patterns runbook Operational procedures decision Architecture decision records api API endpoint documentation See Also CLI Reference Quick Start Guide"
    },
    {
      "id": "reference/cli",
      "title": "CLI Reference",
      "tags": "cli reference commands",
      "content": "CLI Reference The mx CLI provides token-efficient access to your knowledge base. Search Commands mx search Search the knowledge base with hybrid keyword + semantic search. mx search &quot;query&quot; # Hybrid search mx search &quot;docker&quot; --tags=infra # Filter by tag mx search &quot;api&quot; --mode=semantic # Semantic only mx search &quot;api&quot; --mode=keyword # Keyword only mx search &quot;query&quot; --limit=20 # More results mx search &quot;query&quot; --min-score=0.5 # Only confident results mx search &quot;query&quot; --content # Include full content mx search &quot;query&quot; --strict # No semantic fallback mx search &quot;query&quot; --scope=project # Project KB only mx search &quot;query&quot; --include-neighbors mx search &quot;query&quot; --include-neighbors --neighbor-depth=2 mx search &quot;query&quot; --terse # Paths only mx search &quot;query&quot; --full-titles # Show untruncated titles mx search &quot;query&quot; --json # JSON output Options: --tag, --tags : Filter by tags (comma-separated) --mode : Search mode (hybrid, keyword, semantic) --limit, -n : Maximum results (default: 10) --min-score : Minimum score threshold (0.0-1.0) --content : Include full document content in results (replaces snippet) --strict : Disable semantic fallback --terse : Output paths only --full-titles : Show full titles without truncation --scope : Limit to KB scope (project or user) --include-neighbors : Include semantic links + typed relations + wikilinks --neighbor-depth : Neighbor traversal depth (default: 1) --json : JSON output Content Flag Behavior: When --content is used: JSON output : Returns content field with full document text instead of snippet Table output : Shows the standard results table followed by a content section displaying full text for each result Without --content , only a brief snippet is shown (default behavior). Notes: Query cannot be empty. An error is returned for empty or whitespace-only queries. Understanding Search Scores All search scores are normalized to 0.0-1.0 (higher = better match). Score Range Confidence Interpretation &gt;= 0.7 High Strong keyword or semantic match. Trust this result. 0.4 - 0.7 Moderate Partial match. Worth reviewing but may be tangential. &lt; 0.4 Weak Tangential relevance only. May be noise. How scores are calculated: Hybrid mode (default): Combines keyword and semantic search using Reciprocal Rank Fusion (RRF). Results appearing in both rankings score higher. Keyword mode : BM25 text matching. Exact term matches matter most. Semantic mode : Cosine similarity of embeddings. Conceptual meaning matters more than exact words. Score adjustments: After initial ranking, scores receive context-aware boosts: Tag match : +0.05 per matching tag (e.g., searching &quot;python&quot; boosts entries tagged &quot;python&quot;) Project context : +0.15 if entry's source_project matches your current project Boost paths : +0.12 if entry matches patterns in boost_paths in your .kbconfig Scores are re-normalized to 0-1 after boosts, so the top result is always 1.0. Recommended thresholds: # High-confidence results only (for automated workflows) mx search &quot;deployment&quot; --min-score=0.7 # Moderate confidence (good default for exploration) mx search &quot;deployment&quot; --min-score=0.4 # All results (when you want broad coverage) mx search &quot;deployment&quot; mx history View and re-run past searches. mx history # Show last 10 searches mx history -n 20 # Show last 20 mx history --rerun 1 # Re-run most recent mx history --clear # Clear history Read Commands mx get Read a knowledge base entry. mx get tooling/my-entry.md # Full entry by path mx get tooling/my-entry.md --metadata # Metadata only mx get tooling/my-entry.md --json # JSON output mx get --title=&quot;Docker Guide&quot; # Get by title mx get --title &quot;Python Tooling&quot; # Get by title (alt form) Options: --title : Get entry by title instead of path (case-insensitive) --metadata, -m : Show only metadata --json : JSON output Title Lookup Behavior: If one match found: returns that entry If multiple matches: shows error with candidate paths If no match: shows error with similar title suggestions mx list List entries with optional filters. mx list # All entries mx list --tags=infrastructure # Filter by tag mx list --category=tooling # Filter by category mx list --limit=50 # More results mx list --full-titles # Show untruncated titles mx list --scope=project # Project KB only mx tree Display directory structure. mx tree # Full tree mx tree tooling # Specific path mx tree --depth=2 # Limit depth mx tree --scope=project # Project KB only Write Commands mx add Create a new entry. mx add --title=&quot;My Entry&quot; --tags=&quot;foo,bar&quot; --category=tooling --content=&quot;...&quot; mx add --title=&quot;...&quot; --tags=&quot;...&quot; --category=... --file=content.md cat content.md | mx add --title=&quot;...&quot; --tags=&quot;...&quot; --category=... --stdin Required: --title : Entry title --tag, --tags : Tags (comma-separated) --category : Target directory (required unless primary is set in .kbconfig ) Common options: --scope : Target KB scope (project or user) --keywords : Key concepts for semantic linking (comma-separated) --semantic-links : Set semantic links as JSON array mx replace Replace content or metadata in an existing entry. mx replace path/entry.md --tags=&quot;new,tags&quot; mx replace path/entry.md --content=&quot;New content&quot; mx replace path/entry.md --file=new-content.md Note: For appending content, use mx append . For surgical edits, use mx patch . mx patch Surgical find-replace edits. mx patch path/entry.md --find=&quot;old text&quot; --replace=&quot;new text&quot; mx patch path/entry.md --find=&quot;TODO&quot; --replace=&quot;DONE&quot; --replace-all mx patch path/entry.md --find=&quot;...&quot; --replace=&quot;...&quot; --dry-run Intent Detection: If you use flags that suggest a different command (e.g., --content without --find ), the CLI will suggest the correct command: $ mx patch entry.md --content='new stuff' Error: --find is required for find-replace operations. Did you mean: - To append content: mx append 'Title' --content='...' - To replace text: mx patch entry.md --find 'x' --replace 'y' - To overwrite entry: mx replace entry.md --content='...' mx append Append content to existing entry by title, or create new if not found. mx append &quot;Daily Log&quot; --content=&quot;Session summary&quot; mx append &quot;API Docs&quot; --file=api.md --tags=&quot;api,docs&quot; mx append &quot;Debug Log&quot; --content=&quot;...&quot; --no-create # Error if not found cat notes.md | mx append &quot;Meeting Notes&quot; --stdin --tags=&quot;meetings&quot; mx delete Delete an entry. mx delete path/entry.md mx delete path/entry.md --force # Delete even with backlinks mx quick-add Quickly add content with auto-generated metadata. mx quick-add --stdin # Paste content, auto-generate all mx quick-add -f notes.md # From file with auto metadata mx quick-add --content &quot;...&quot; --confirm # Auto-confirm creation echo &quot;...&quot; | mx quick-add --stdin --json mx ingest Ingest a markdown file into the KB, adding frontmatter if missing. mx ingest notes.md # Auto-detect title/tags mx ingest draft.md --title=&quot;My Entry&quot; # Override title mx ingest doc.md --tags=&quot;api,docs&quot; # Set tags mx ingest doc.md --directory=&quot;guides&quot; # Place in guides/ mx ingest doc.md --scope=project # Project KB only mx ingest doc.md --dry-run # Preview changes Analysis Commands mx health Audit KB for problems. mx health mx health --json Checks for: Orphaned entries (no backlinks) Broken links Stale content (&gt;90 days) Missing frontmatter mx relations-lint Warn on unknown or inconsistent typed relation types (non-blocking by default). mx relations-lint mx relations-lint --json mx relations-lint --strict # Exit non-zero if issues found Use mx relations-lint to align relation types with the canonical taxonomy. mx hubs Show most connected entries. mx hubs mx hubs --limit=5 mx suggest-links Find semantically related entries. mx suggest-links path/entry.md mx suggest-links path/entry.md --limit=10 mx tags List all tags with counts. mx tags mx tags --min-count=3 Browse Commands mx info Show KB configuration and stats. mx info mx info --json Alias: mx config mx whats-new Show recently modified entries. mx whats-new # Last 30 days mx whats-new --days=7 # Last week mx whats-new --scope=project # Project KB only mx whats-new --limit=20 # More results Project Setup Commands mx init Initialize a knowledge base (project or user scope). mx init # Project KB in ./kb/ mx init --user # User KB in ~/.memex/kb/ mx init --path docs/kb # Custom location mx init --force # Reinitialize existing mx init --json # JSON output Options: --path, -p : Custom location for local KB (default: kb/) --user, -u : Create user KB at ~/.memex/kb/ --force, -f : Reinitialize existing KB --json : JSON output What gets created: Project: kb/README.md and .kbconfig at the project root User: ~/.memex/kb/README.md and ~/.memex/kb/.kbconfig When to use: Starting a new project that needs project-specific documentation Creating a knowledge base that should be versioned with the code Setting up for GitHub Pages publishing mx context Show or validate project KB configuration. mx context # Show current config mx context show # Same as above mx context validate # Validate config paths mx context validate --json # JSON output Note: .kbconfig is created by mx init . Automation Commands mx batch Execute multiple KB operations in a single invocation. mx batch &lt;&lt; 'EOF' add --title='Note 1' --tags='tag1' --category=tooling --content='Content' search 'api' EOF Reads commands from stdin (or --file ) and returns JSON results. Agent Memory Commands mx memory Agent memory for AI coding assistants. mx memory # Show memory status mx memory init # Enable memory for this project mx memory add &quot;note&quot; # Add a manual memory note mx memory inje"
    },
    {
      "id": "reference/chat-gpt-5.2-pro-dense-memory-signals-reference",
      "title": "ChatGPT 5.2 Pro Dense Memory Signals Reference",
      "tags": "reference ai memory signals",
      "content": "My prompt: Research Prompt: Dense Memory Signals for Agent Context Injection Context I'm building an agent memory system for LLM-based coding assistants (like Claude Code, Cursor, Codex). The core problem: when an agent's context window is cleared or a new session starts, valuable context is lost—decisions made, gotchas discovered, work in progress, etc. Dense memory signals are compact, high-information-density snippets that can be injected at session start to restore context without consuming many tokens. The goal is to find signals with the best ratio of usefulness to token cost. What I Already Know I've researched existing tools and identified some signals: Tier 1 (High Value, Low Tokens): Decisions made + rationale (~15 tokens, critical—LLM can't infer this) Gotchas/warnings discovered (~10 tokens, prevents repeated mistakes) User corrections (~10 tokens, direct mistake prevention) Git staged files (~5 tokens, interrupted work signal) Git stash contents (~5 tokens, parked work context) Branch name (~3 tokens, feature context) Build status (~3 tokens, &quot;FAILING&quot; changes everything) Files read but NOT modified (~10 tokens, research context) Tier 2 (Medium Value): Files touched with context (~15 tokens) Summary of what was done (~25 tokens) Questions the LLM asked (~15 tokens) Tool usage patterns (~10 tokens) Errors encountered (~15 tokens) TODOs added/removed (~10 tokens) Existing tools' approaches: claude-mem: Causal observations (what led to what) Mem0: Extract facts, not transcripts (~90% token reduction) A-Mem (NeurIPS 2025): Atomic notes with metadata, embeddings, links (85% reduction) Letta/MemGPT: Message buffer + core memory blocks + archival What I Need Help With Identify non-obvious signals I'm missing: What context does a developer carry in their head that an LLM loses between sessions? What signals are available from git, IDE state, project structure, CI/CD, etc. that aren't commonly captured? What &quot;meta&quot; signals exist (patterns across sessions, user habits, team conventions)? Evaluate signal properties: For each signal you identify, estimate: Token cost: How many tokens to represent it? Information value: High/Medium/Low—how much does this help the agent? Extraction difficulty: Easy (automated) / Medium (heuristics) / Hard (requires LLM inference) Uniqueness: Can the LLM infer this from code, or is it hidden context only humans know? Consider the &quot;anti-signals&quot;: What signals seem useful but actually waste tokens or mislead the agent? What context should NOT be injected (too noisy, too stale, inferrable from code)? Explore scaling strategies: When an agent touches 50+ files across many sessions, raw signal lists explode. How should signals be: Summarized hierarchically (file → directory → project level) Decayed over time (recent = full detail, old = compressed) Clustered semantically (group related signals) Filtered by relevance to current task Cross-domain inspiration: What do human developers do to restore context? (Notes, git commit messages, PR descriptions, etc.) What do other domains do for context continuity? (Medical handoffs, shift changes, project management) What research exists on human memory cues that could apply? Constraints Token budget: The injection should be ~100-300 tokens for a &quot;dense&quot; format, expandable to ~500-1000 for &quot;rich&quot; format Extraction source: Primarily git history, file system, session logs (parsed conversation transcripts), and project config No user action required: Signals should be automatically extractable without user tagging or annotation Desired Output Please provide: A prioritized list of signals I haven't considered, with the properties above Critiques of my current signal list—what's overrated, underrated, or missing nuance? A proposed signal taxonomy—how would you categorize signals (by source? by value? by extraction method?) Scaling recommendations for when signal volume gets high Any relevant research, papers, or prior art I should investigate Example Output Format Signal: [Name] Description : What this captures Token cost : ~N tokens Value : High/Medium/Low—why Extraction : Easy/Medium/Hard—how Uniqueness : Can LLM infer this? Why/why not? Example : &quot;Branch: feat/caching | Stash: 'WIP: redis connection pooling'&quot; That prompt is self-contained and should elicit useful research contributions from other LLMs. You could also ask specific follow-up questions like: &quot;Focus specifically on git-derived signals I might be missing&quot; &quot;What signals exist in IDE state (cursor position, open tabs, breakpoints)?&quot; &quot;How do enterprise knowledge management systems handle context handoff?&quot; ======++++++REPLY+++++===== You’re already thinking in the right direction: most “lost context” isn’t code , it’s the invisible stuff a dev (or agent) just learned: what we’re trying to accomplish, what we tried that failed, what we decided, what’s currently broken, and what the next step is . Below are (1) high-leverage signals you’re missing, (2) critiques of your list, (3) a taxonomy that’s practical for extraction + ranking, (4) scaling tactics when the signal list explodes, and (5) research/prior art worth mining. 1) Prioritized “missing” dense signals I’m prioritizing by usefulness ÷ tokens , and by “can’t be inferred from code”. Signal: Next Step / Open Loop Description : The single most valuable handoff cue: what the agent was about to do next (or what remains undone). Token cost : ~10–25 Value : High — eliminates “re-figure out what I was doing” time. Extraction : Medium — infer from final transcript turns + last tool calls + last git changes. Uniqueness : High — not reliably inferable from repo state. Example : “Next: fix failing TestCacheEviction::ttl_expired by adjusting clock mock in cache_test.go .” Signal: Current Blocker + Why Description : What’s preventing progress right now (missing secret, flaky test, unclear spec, reproducer unavailable). Token cost : ~10–25 Value : High — prevents the classic “repeat same dead end”. Extraction : Medium — parse last error outputs + “I can’t because …” statements. Uniqueness : High — often external or transient. Example : “Blocked: can’t run integration tests— REDIS_URL missing locally.” Signal: Merge/Rebase/Conflict State Description : Whether git is mid-merge/rebase/cherry-pick and which files are conflicted. Token cost : ~8–20 Value : High — changes everything about the next actions. Extraction : Easy — detect .git/MERGE_HEAD , .git/rebase-apply , git status . Uniqueness : High — not in code; pure workflow state. Example : “REBASE in progress; conflicts: src/api.ts , package-lock.json .” Signal: Dirty Working Tree Diffstat (Staged + Unstaged + Untracked) Description : A compact snapshot of “how much is in flight” + where, without listing every file. Token cost : ~10–20 Value : High — signals incomplete work and risk. Extraction : Easy — git status --porcelain , git diff --stat , git diff --cached --stat . Uniqueness : Medium — partially inferable by looking at repo, but not from code alone. Example : “Dirty: staged 2 files (+45/-10), unstaged 5 (+120/-40), untracked: 1.” Signal: Ahead/Behind + Unpushed Commits Titles Description : Whether local work is unpublished + what it is (commit subject lines). Token cost : ~10–25 (top 1–3 commit subjects) Value : High — tells the agent “the story so far”. Extraction : Easy — git status -sb , git log @{u}.. --oneline -n 3 . Uniqueness : Medium — code contains changes, but intent is compressed in subjects. Example : “Ahead 2: Add redis pool , Fix TTL race in eviction .” Signal: Last Command(s) Run + Exit Code Description : The most recent build/test/lint/dev-server command and whether it succeeded. Token cost : ~10–30 Value : High — beats a vague “build failing”. Extraction : Medium — capture terminal/tool logs; normalize + redact paths. Uniqueness : High — not deducible from code. Example : “Last: pytest -k cache → FAIL (3), ruff check → OK.” Signal: Failure Signature (Canonical Error Fingerprint) Description : One-line normalized summary of the dominant failure (error type + location + key message). Token cost : ~8–18 Value : High — rehydrates debugging context instantly. Extraction : Medium — parse logs; dedupe stack traces; pick top recurring signature. Uniqueness : High — ephemeral runtime info. Example : “FAIL: KeyError: 'user_id' at src/auth.py:112 (in LoginHandler ).” Signal: Active Working Set (Open Tabs + Primary Cursor Location) Description : The “mental desktop”: which files were open and where the cursor last was. Token cost : ~15–40 (top 3–8 files + 1–3 cursor spots) Value : High — humans use this as their fastest context cue. Extraction : Easy/Medium — easy if IDE exposes state; medium if inferred from recent reads/edits. Uniqueness : High — not in repo; purely session state. Example : “Focus: src/cache/pool.ts:188 ; tabs: pool.ts , redis.ts , cache.test.ts .” Why I’m so confident this matters: task-context tooling in IDEs (e.g., Eclipse Mylyn / Mylar) explicitly models “task context” from developer interaction and uses it to focus what you see. That’s basically a human “dense memory signal” system. ( Eclipse ) Signal: Navigation Trace (Symbols/Paths Investigated) Description : A compressed “investigation trail”: key symbols, call chain, or files traversed during debugging. Token cost : ~15–40 Value : High — prevents re-reading the same rabbit hole; restores the mental model. Extraction : Medium — from IDE navigation events / “go to definition” logs / conversation + file reads. Uniqueness : Medium/High — code exists, but “what we already checked” isn’t. Example : “Trace: RequestHandler→AuthSvc→TokenRepo ; checked JWTClaims parsing; not culprit.” Signal: Environment / Toolchain Fingerprint Description : The minimal environment facts that commonly break builds: language versions, package manager, OS/arch, container vs host. Token cost : ~10–30 Value : High (conditional) — huge when mismatch"
    },
    {
      "id": "reference/focusgroup-evaluation-mx-cli-discoverability-2026-01",
      "title": "Focusgroup Evaluation: mx CLI Discoverability (2026-01)",
      "tags": "focusgroup evaluation mx cli",
      "content": "Focusgroup Evaluation: mx CLI Discoverability Date: 2026-01-12 Agents: Opus 4.5 (Claude), Codex (OpenAI) Mode: Exploration (agents ran commands interactively) Summary Two AI agents evaluated the mx CLI for discoverability and agent-friendliness. They ran mx prime , then attempted common operations without reading --help . Key Findings What Works Well Feature Assessment mx prime Excellent - provides quick workflow context mx batch Outstanding - JSON output, multiple ops, exactly what agents need mx schema Very useful for programmatic discovery mx search Clean tabular output with scores and confidence mx health Good actionable diagnostics Tag suggestions Helpful after mx add Bugs Found Priority Issue Impact P0 mx info shows 0 entries when entries exist Misleading stats P0 mx publish crashes with AttributeError Feature broken P1 --json-errors doesn't produce JSON Blocks programmatic error handling P1 mx history always empty Feature broken P1 mx tags/hubs throw tracebacks without KB Inconsistent errors P2 mx replace --tags requires --content Unnecessary friction P2 \\n in --content stored literally Escape handling missing P2 mx whats-new datetime crash (intermittent) Fragile datetime handling Documentation Issues mx add requires --category but docs show it as optional --tag vs --tags inconsistency in examples (fixed 2026-01-12) Config naming consistency: docs should refer to .kbconfig everywhere Agent-Friendliness Recommendations Fix --json-errors - Agents need structured errors Make error handling consistent - All commands should give friendly errors or structured JSON mx batch is the gold standard - Other commands should follow its patterns Document --stdin for formatted content - Since --content doesn't interpret escapes Best Practices for Agents # Use batch for multiple operations (structured JSON output) mx batch &lt;&lt; 'EOF' add --title='...' --tags='...' --category=tooling --content='...' search 'query' EOF # Use --stdin for content with formatting echo -e &quot;Line 1\\n\\nLine 2&quot; | mx add --title=&quot;...&quot; --tags=&quot;...&quot; --category=... --stdin # Check for KB first mx info &gt;/dev/null 2&gt;&amp;1 || mx init Session Details Session ID: 218cdae5 Duration: ~11 minutes Log: ~/.local/share/focusgroup/logs/20260112-218cdae5.json"
    },
    {
      "id": "reference/dense-memory-signals-research",
      "title": "Dense Memory Signals Research",
      "tags": "agent-memory research memory signals",
      "content": "Dense Memory Signals Research Research into what contextual signals can be captured and injected to help agents maintain context across sessions. Goal: identify high-value, low-token signals. What Existing Tools Capture Tool Primary Signals Token Cost claude-mem Tool observations, semantic summaries, session data 50-1000 tokens (progressive) basic-memory Structured notes, relations, knowledge graph links Medium Mem0 Facts extracted from conversations (not transcripts) ~90% reduction vs full context Letta/MemGPT Message buffer, core memory blocks, recall history, archival knowledge Variable A-Mem (NeurIPS 2025) Atomic notes with keywords, tags, context, embeddings, links 1,200-2,500 tokens vs 16,900 baseline Signal Tiers Tier 1: High Value / Low Token Cost (The Gold) Signal Tokens Value Notes Decisions made + rationale ~15 Critical Claude can't infer this Gotchas/warnings discovered ~10 Critical &quot;X looks right but fails because Y&quot; What user corrected ~10 Critical Direct mistake prevention Files in staging (git) ~5 High Interrupted work signal Stash contents ~5 High Parked work context Branch name ~3 High Feature context in 3 tokens Build status ~3 High &quot;FAILING&quot; changes everything Files read but NOT modified ~10 High Research context Tier 2: Medium Value / Medium Token Cost Signal Tokens Value Files touched (with context) ~15 High Summary of what was done ~25 High Questions Claude asked ~15 High Tool usage patterns ~10 Medium Errors encountered ~15 Medium TODOs added/removed ~10 Medium Tier 3: Novel/Non-Obvious Signals Signal Tokens Notes Causal chains ~30 &quot;X led to Y which caused Z&quot; Semantic links ~20 Related memories by meaning User intent shifts ~15 &quot;Started on X, pivoted to Y&quot; Failed tool calls ~10 What approaches didn't work Search queries with no results ~10 Dead ends to avoid Key Research Insights Atomic &gt; Monolithic : A-Mem treats memories as single, self-contained units with rich metadata. This enables 85% token reduction. Causality matters : claude-mem's innovation is that every observation includes what came before and after. The LLM sees causality, not snapshots. Facts &gt; Transcripts : Mem0 extracts relevant facts, not full transcripts—reducing tokens ~90%. Scaling Strategies As agents become more capable and touch more files, signal count explodes. Strategies: 1. Hierarchical Summarization File-level → Directory-level → Project-level &quot;Modified 12 files in src/api/&quot; vs listing all 12 2. Recency Decay Recent signals: full detail Older signals: compressed summaries Ancient signals: existence only (&quot;worked on caching 2 weeks ago&quot;) 3. Semantic Clustering Group related files by purpose, not path &quot;Modified auth flow (5 files)&quot; vs listing each 4. Activity Thresholds Below threshold: list all Above threshold: summarize + highlight outliers &quot;Touched 47 files, notably: new cache.ts, heavily modified api.ts&quot; 5. Delta-Only for Repeats First session: full context Subsequent: only what changed since last injection 6. User-Guided Salience Let user mark signals as important/irrelevant Learn what matters per-project Proposed Dense Injection Format [Session: 2d ago | Branch: feat/caching | Build: PASSING] RESUMED: Implementing Redis cache layer TOUCHED: cache.ts(added), api.ts(modified), redis.ts(read-only) STAGED: cache.ts (incomplete - needs TTL logic) DECISIONS: - Chose Redis over Memcached (team familiarity) GOTCHAS: - redis.ts exports are CJS not ESM CORRECTIONS: - User: &quot;Use connection pooling, not single client&quot; Estimated: ~120 tokens with very high information density. Sources claude-mem GitHub basic-memory Docs Mem0 GitHub Letta Agent Memory A-Mem Paper (NeurIPS 2025) 2026 Enterprise Memory Stack"
    },
    {
      "id": "guides/mcp-setup",
      "title": "MCP Server Setup (Deprecated)",
      "tags": "mcp claude-desktop setup configuration deprecated",
      "content": "MCP Server Setup (Deprecated) Deprecated: Memex no longer ships or supports an MCP server. Use the mx CLI instead. If you previously configured an MCP server, remove the memex entry from your Claude Desktop or Claude Code config to avoid startup errors. Use the CLI Instead Installation Guide - Install mx AI Agent Integration - Configure agents CLI Reference - Command list"
    },
    {
      "id": "guides/ai-integration",
      "title": "AI Agent Integration",
      "tags": "ai claude-code codex agents integration",
      "content": "AI Agent Integration Memex is designed for AI coding assistants. The CLI is the recommended interface. Claude Code Permission Setup Add to .claude/settings.local.json : { &quot;permissions&quot;: { &quot;allow&quot;: [&quot;Bash(mx:*)&quot;] } } This grants Claude Code permission to run any mx command without prompting. Session Hooks For automatic context injection, use hooks: { &quot;hooks&quot;: { &quot;SessionStart&quot;: [{ &quot;hooks&quot;: [{ &quot;type&quot;: &quot;command&quot;, &quot;command&quot;: &quot;mx session-context&quot; }] }] } } Write the hook into .claude/settings.json : mx session-context --install The mx session-context command: Injects project-relevant KB context at session start Provides a concise workflow reminder plus related entries Claude Code Plugin (Optional) This repo ships a Claude Code plugin manifest in .claude-plugin/ . From the repo root, add the local marketplace and install the plugin: /plugin marketplace add ./.claude-plugin/marketplace.json /plugin install memex@memex Restart Claude Code after installing or updating the plugin. Workflow Pattern # Before implementing: search KB for existing patterns mx search &quot;authentication patterns&quot; # During work: add discoveries for future sessions mx add --title=&quot;OAuth2 Setup&quot; --tags=&quot;auth,patterns&quot; --category=patterns \\ --content=&quot;...&quot; Codex CLI Codex can use memex via shell commands in AGENTS.md: ## Knowledge Base Search organizational knowledge before implementing: - `mx search &quot;query&quot;` - Find existing patterns - `mx get path/entry.md` - Read specific entry - `mx add --title=&quot;...&quot; --tags=&quot;...&quot; --category=... --content=&quot;...&quot;` - Add discoveries Codex Skills (Optional) This repo includes a Memex skill at skills/memex-kb-usage/ . Codex loads skills from $CODEX_HOME/skills (defaults to ~/.codex/skills ). Copy or symlink the skill into that directory and restart Codex. Example: mkdir -p ~/.codex/skills cp -r skills/memex-kb-usage ~/.codex/skills/ Other AI Agents Any agent with shell access can use the mx CLI. Common Patterns # Check for relevant knowledge before implementing mx search &quot;deployment strategies&quot; # Add discoveries for future sessions mx add --title=&quot;API Rate Limiting&quot; \\ --tags=&quot;api,patterns&quot; \\ --category=patterns \\ --content=&quot;...&quot; # View recent project KB updates mx whats-new --scope=project --days=7 # Quick status check mx info Search Strategy Before implementing : Search for existing patterns When stuck : Search for troubleshooting guides After solving : Add solution to KB When to Search KB Looking for organizational patterns or guides Before implementing something that might exist Understanding infrastructure or deployment Troubleshooting known issues When to Contribute Discovered reusable pattern or solution Troubleshooting steps worth preserving Infrastructure or deployment knowledge Project-specific conventions Project Configuration Configure project-specific KB settings in .kbconfig : # In your project directory cat &lt;&lt;'EOF' &gt; .kbconfig kb_path: ./kb primary: design boost_paths: - design/* default_tags: - memex EOF This .kbconfig file: Points mx at the project KB ( kb_path ) Routes new entries to projects/&lt;name&gt; by default ( primary ) Boosts project entries in search results ( boost_paths ) Suggests project-specific tags ( default_tags ) Best Practices Search before creating - Avoid duplicate entries Tag consistently - Use mx tags to see existing tags Link related entries - Use [[path/to/entry]] syntax Keep entries focused - One topic per entry Update, don't duplicate - Append to existing entries See Also CLI Reference Entry Format"
    },
    {
      "id": "guides/quick-start",
      "title": "Quick Start Guide",
      "tags": "getting-started tutorial basics",
      "content": "Quick Start Guide Get productive with memex in 5 minutes. 1. Set Up Your Knowledge Base Choose a KB scope: # Project KB (shared with your repo) mx init # User KB (personal, available everywhere) mx init --user Optional overrides: # Point to an existing user KB in a custom location export MEMEX_USER_KB_ROOT=~/kb # Store indices outside the KB directory export MEMEX_INDEX_ROOT=~/.memex-indices 2. Create Your First Entry mx add \\ --title=&quot;Git Stash Workflow&quot; \\ --tags=&quot;git,workflow,cli&quot; \\ --category=tooling \\ --content=&quot;# Git Stash Workflow Quick save work in progress: \\`\\`\\`bash git stash push -m 'WIP: feature X' git stash list git stash pop \\`\\`\\` Use \\`git stash apply\\` to keep the stash after applying.&quot; This creates tooling/git-stash-workflow.md in your KB. Note: mx add requires --category unless you set primary in .kbconfig (project) or your user KB .kbconfig . 3. Search for It # Keyword search mx search &quot;stash&quot; # With semantic search (if installed) mx search &quot;save work in progress&quot; # Filter by tag mx search &quot;git&quot; --tags=workflow # Limit to a specific KB mx search &quot;stash&quot; --scope=project mx search &quot;stash&quot; --scope=user When both KBs exist, results are prefixed with @project/... and @user/... . 4. Read It Back # Full entry with content mx get tooling/git-stash-workflow.md # Metadata only mx get tooling/git-stash-workflow.md --metadata 5. Check KB Health mx health Audits your KB for: Missing frontmatter Broken links Orphaned entries Index sync issues Essential Commands Command Description mx search &quot;query&quot; Search the KB mx get path/entry.md Read an entry mx add --title=&quot;...&quot; --tags=&quot;...&quot; --category=... Create entry mx tree Browse structure mx tags List all tags mx whats-new Recent changes mx health Audit KB Next Steps CLI Reference - Full command documentation Entry Format - Frontmatter and linking AI Agent Integration - Use with Claude Code"
    },
    {
      "id": "guides/installation",
      "title": "Installation Guide",
      "tags": "installation setup getting-started",
      "content": "Installation Guide Memex installs with semantic search enabled by default (ChromaDB + sentence-transformers). Expect a larger install footprint and a first-run model download. Install (Recommended) # With uv (recommended) uv tool install memex-kb # With pip pip install memex-kb # Verify installation mx --version This includes: Hybrid search (keyword + semantic) ChromaDB for vector storage sentence-transformers for embeddings CPU-only PyTorch (~500MB additional) First search downloads embedding model (~100MB) From Source For development or customization: git clone https://github.com/chriskd/memex-kb.git cd memex # Runtime dependencies uv sync # Dev dependencies uv sync --dev GPU Support (Optional) If you have an NVIDIA GPU and want CUDA acceleration: uv sync --index pytorch-gpu=https://download.pytorch.org/whl/cu124 Platform Notes ARM64 (Apple Silicon) : ChromaDB capped at &lt;1.0.0 for onnxruntime compatibility CPU-only default : PyTorch configured for CPU to minimize install size Python requirement : 3.11 or higher Next Steps After installation: Quick Start Guide - Create your first KB CLI Reference - Full command documentation AI Agent Integration - Configure AI assistants"
    },
    {
      "id": "design/chunking-system-design",
      "title": "Chunking System Design",
      "tags": "memex architecture chunking a-mem",
      "content": "Chunking System for Memex Goal Add configurable chunking that stores chunks only in ChromaDB while keeping markdown files whole. This enables: Precise snippet retrieval (independent of A-Mem) A-Mem parity when combined with evolution features LoCoMo-style evaluation for testing efficacy Key Design Decisions Decision Choice Rationale Chunk storage ChromaDB only No file clutter, markdown stays human-readable Default strategy headers Backward compatible; easy opt-in to paragraph Priority strategy sentences Closest to A-Mem for parity testing Frontmatter keywords Document-level only Chunk keywords live in ChromaDB metadata Search default Dedupe by file Backward compatible, --show-chunks for granular Eval suite Specs now, impl Phase 2 Ship chunking first, validate after Configuration # .kbconfig chunking: enabled: true strategy: paragraph # headers, paragraph, semantic, sentences max_chunk_tokens: 256 # For semantic strategy overlap_tokens: 32 # Context continuity min_chunk_tokens: 20 # Avoid tiny chunks Implementation Phases Phase 1: Configuration (config.py) Add ChunkingConfig dataclass Add get_chunking_config() loader Update .kbconfig template in cli.py Phase 2: Chunking Strategies (parser/) Create src/memex/parser/chunking.py module Implement strategies: chunk_by_headers() - refactor existing _chunk_by_h2() chunk_by_paragraph() - split on double newlines chunk_by_semantic() - sentence boundaries + token limits chunk_by_sentences() - individual sentences (A-Mem-like) Add sentence splitting utility with offset tracking Phase 3: Index Schema Updates (indexer/) ChromaDB ID format : path#chunk_{idx} or path#{section}#chunk_{idx} New metadata fields : chunk_idx , parent_section , chunk_strategy , start_offset , end_offset Update delete logic : Query by path metadata, not ID pattern Phase 4: Models Update (models.py) Extend DocumentChunk with: chunk_idx: int parent_section: str | None chunk_strategy: str start_offset: int end_offset: int Phase 5: Search &amp; CLI Updates Add --show-chunks flag to search command Update SearchResult with chunk metadata JSON output includes full chunk info Phase 6: Migration &amp; Reindex Store chunking_strategy in collection metadata Detect strategy mismatch, warn user Add mx reindex --force-rechunk command Critical Files File Changes src/memex/config.py Add ChunkingConfig dataclass src/memex/parser/chunking.py NEW - chunking strategies src/memex/parser/markdown.py Refactor to use chunking module src/memex/indexer/chroma_index.py New ID format, metadata fields src/memex/indexer/whoosh_index.py Match schema changes src/memex/models.py Extend DocumentChunk src/memex/cli.py Add --show-chunks, config template tests/test_chunking.py NEW - chunking tests Semantic Chunking Algorithm 1. Split content into sentences with character offsets 2. Accumulate sentences until max_tokens reached 3. On overflow: create chunk, start new with overlap 4. Handle edge cases: code blocks, long sentences A-Mem Integration Points When amem.enabled + chunking.enabled : Evolution updates chunk_keywords in ChromaDB (not frontmatter) should_evolve decision per chunk relationship Chunk-level semantic links possible Verification Unit tests : Each strategy produces expected chunks Integration : mx reindex --force-rechunk with each strategy Search : mx search &quot;query&quot; --show-chunks returns chunk-level results Migration : Strategy change detected, warning shown Phase 2: Evaluation Suite (Specification) Implementation deferred to Phase 2. This spec enables KB entry and beads creation now. Goal Test A-Mem evolution efficacy using LoCoMo-style methodology, adapted for document-centric KB. Metrics (from A-Mem paper) # src/memex/eval/metrics.py - exact_match: bool # Exact string equality - f1: float # Token overlap F1 score - rouge1_f, rouge2_f, rougeL_f: float # ROUGE scores - bleu1-4: float # BLEU scores - bert_f1: float # BERTScore F1 - meteor: float # METEOR score - sbert_similarity: float # Sentence-BERT cosine similarity Test Categories (from LoCoMo) Category Tests Memex Equivalent Multi-hop Connecting multiple memories Cross-document links via evolution Temporal Date/time reasoning Entry created/updated metadata Open-domain General retrieval Standard search Single-hop Direct retrieval Exact keyword match Adversarial &quot;Not mentioned&quot; detection No-result handling Evaluation Flow 1. Create test KB with known entries 2. Add entries, let evolution run 3. Query with test questions 4. Compare retrieved content to expected answers 5. Calculate metrics across categories CLI Command (Phase 2) mx eval --dataset tests/eval/locomo_adapted.json mx eval --compare-strategies headers,paragraph,sentences mx eval --with-evolution --without-evolution # A/B test Adaptation for Document-Centric KB LoCoMo (conversational): Each utterance = 1 memory note ~1-2 sentences per note Memex adaptation: Each paragraph/section = 1 chunk (with sentences strategy) Test questions query for specific facts within chunks Evolution should improve retrieval by updating chunk keywords Test Dataset Format { &quot;entries&quot;: [ { &quot;path&quot;: &quot;test/doc1.md&quot;, &quot;content&quot;: &quot;...&quot;, &quot;chunks_expected&quot;: 5 } ], &quot;qa&quot;: [ { &quot;question&quot;: &quot;What configuration is needed for X?&quot;, &quot;answer&quot;: &quot;Set Y=Z in config&quot;, &quot;category&quot;: 4, &quot;expected_chunk&quot;: &quot;test/doc1.md#chunk_2&quot; } ] } Beads Tracking Phase 1 (This Work) voidlabs-kb-jrzy - EPIC : Configurable chunking system for ChromaDB voidlabs-kb-mhoy - Add ChunkingConfig to config.py voidlabs-kb-k36r - Implement chunking strategies module voidlabs-kb-xnxc - Update ChromaDB/Whoosh schema for chunks voidlabs-kb-b24w - Add --show-chunks to mx search voidlabs-kb-g9ic - Add mx reindex --force-rechunk Phase 2 (Deferred) voidlabs-kb-p74h - Implement evaluation metrics module voidlabs-kb-pio2 - Create adapted LoCoMo test dataset voidlabs-kb-gvpe - Build mx eval command voidlabs-kb-lvg6 - A/B comparison: with/without evolution"
    },
    {
      "id": "design/llm-query-processing-feature-plan",
      "title": "LLM Query Processing Feature Plan",
      "tags": "memex llm feature-plan",
      "content": "Plan: LLM Query Processing for KB Entries Summary Add opt-in LLM processing to KB entries with caching. Supports summarize, synthesize, and answer operations. CLI Interface mx get path/entry.md --llm # Summarize entry mx get path/entry.md --llm --no-cache # Force fresh generation mx search &quot;query&quot; --llm # Synthesize results to answer query mx ask &quot;How do I deploy?&quot; # Shorthand for search + synthesize mx ask &quot;...&quot; --show-sources # Include source paths mx llm-cache stats # Cache statistics mx llm-cache clear # Clear cache Files to Create src/memex/llm_query.py Core LLM operations: content_hash(content: str) -&gt; str - SHA256 for cache keys summarize_entry(entry: KBEntry) -&gt; LLMResponse synthesize_entries(entries: list[KBEntry]) -&gt; LLMResponse answer_query(query: str, entries: list[KBEntry]) -&gt; LLMResponse _truncate_content() - Handle token limits with tiktoken src/memex/llm_cache.py Cache management (stored in .indices/llm_cache.json ): get_cached_response(operation, content_hashes, query) -&gt; CachedLLMResponse | None cache_response(operation, paths, hashes, query, model, response, tokens) clear_cache() -&gt; int cache_stats() -&gt; dict Cache key: sha256(operation + sorted(content_hashes) + query)[:16] Files to Modify src/memex/config.py Add LLMQueryConfig dataclass: @dataclass class LLMQueryConfig: enabled: bool = False model: str = &quot;anthropic/claude-3-5-haiku&quot; max_input_tokens: int = 8000 cache_enabled: bool = True Add get_llm_query_config() (follow get_memory_evolution_config() pattern) src/memex/models.py Add: class LLMResponse(BaseModel): operation: Literal[&quot;summarize&quot;, &quot;synthesize&quot;, &quot;answer&quot;] input_paths: list[str] response: str model: str cached: bool = False token_count: int = 0 src/memex/cli.py Add to get command (line ~1189): --llm flag --no-cache flag Add to search command (line ~973): --llm flag --no-cache flag Add new ask command: Search → load entries → call answer_query() Options: --limit , --show-sources , --no-cache Add llm-cache command group with stats and clear subcommands Config (.kbconfig) llm_query: enabled: true model: anthropic/claude-3-5-haiku max_input_tokens: 8000 cache_enabled: true Cache Format (.indices/llm_cache.json) { &quot;version&quot;: 1, &quot;entries&quot;: { &quot;cache_key_hash&quot;: { &quot;operation&quot;: &quot;summarize&quot;, &quot;input_hashes&quot;: [&quot;sha256...&quot;], &quot;input_paths&quot;: [&quot;path.md&quot;], &quot;query&quot;: null, &quot;model&quot;: &quot;anthropic/claude-3-5-haiku&quot;, &quot;response&quot;: &quot;...&quot;, &quot;created_at&quot;: &quot;2024-01-14T12:00:00Z&quot;, &quot;token_count&quot;: 150 } } } Implementation Order Add LLMQueryConfig to config.py Create llm_cache.py Create llm_query.py Add LLMResponse to models.py Add --llm to mx get Add --llm to mx search Add mx ask command Add mx llm-cache commands Verification # Enable in config echo 'llm_query:\\n enabled: true' &gt;&gt; .kbconfig # Test summarize mx get kb/tooling/beads.md --llm # Test cached response mx get kb/tooling/beads.md --llm # Should say &quot;(cached)&quot; # Test answer query mx ask &quot;How do I track issues?&quot; # Test cache management mx llm-cache stats mx llm-cache clear # Run tests uv run pytest tests/test_llm_query.py -v"
    },
    {
      "id": "design/state-diagrams",
      "title": "MX State Diagrams",
      "tags": "memex architecture flows state-diagrams",
      "content": "MX State Diagrams These diagrams capture the primary MX 0.2.0 user journeys and the key states that govern behavior across CLI, storage, and publishing. Init and KB Discovery mx init (project or user scope) stateDiagram-v2 [*] --&gt; ParseArgs ParseArgs --&gt; InvalidOptions: --user + --path InvalidOptions --&gt; [*] ParseArgs --&gt; SelectTarget SelectTarget --&gt; ExistsCheck ExistsCheck --&gt; ErrorExists: exists and !--force ErrorExists --&gt; [*] ExistsCheck --&gt; CreateKB: not exists or --force CreateKB --&gt; WriteReadme WriteReadme --&gt; WriteConfig WriteConfig --&gt; Done Done --&gt; [*] Notes: Project scope writes .kbconfig at the repo root with kb_path . User scope writes .kbconfig inside ~/.memex/kb/ . KB discovery (reads) + context overlays stateDiagram-v2 [*] --&gt; NeedKB NeedKB --&gt; CheckProject: find .kbconfig up tree (unless MEMEX_SKIP_PROJECT_KB) CheckProject --&gt; ProjectFound: kb_path resolved CheckProject --&gt; CheckUser: not found CheckUser --&gt; UserFound: MEMEX_USER_KB_ROOT or ~/.memex/kb with .kbconfig CheckUser --&gt; NoKB: none ProjectFound --&gt; UseProject UserFound --&gt; UseUser NoKB --&gt; Error stateDiagram-v2 [*] --&gt; WalkUp WalkUp --&gt; LoadContext: .kbconfig found WalkUp --&gt; NoContext: none LoadContext --&gt; [*] NoContext --&gt; [*] Notes: Context overlays supply primary , boost_paths , default_tags , and publish_base_url . Add, Update, Patch mx add (create entry) stateDiagram-v2 [*] --&gt; ResolveKB ResolveKB --&gt; DiscoverContext DiscoverContext --&gt; ChooseTarget ChooseTarget --&gt; UseDirectory: --directory ChooseTarget --&gt; UseCategory: --category ChooseTarget --&gt; UsePrimary: context.primary ChooseTarget --&gt; ErrorMissingTarget: none UseDirectory --&gt; ValidateDirectory UseCategory --&gt; EnsureCategoryDir UsePrimary --&gt; EnsurePrimaryDir ValidateDirectory --&gt; ValidateTags EnsureCategoryDir --&gt; ValidateTags EnsurePrimaryDir --&gt; ValidateTags ValidateTags --&gt; SlugifyTitle SlugifyTitle --&gt; ExistsCheck ExistsCheck --&gt; ErrorExists: file exists ExistsCheck --&gt; WriteEntry: available WriteEntry --&gt; IndexEntry IndexEntry --&gt; MaybeSemanticLinks MaybeSemanticLinks --&gt; Done Done --&gt; [*] Notes: --category (or --directory ) is required unless primary is set in .kbconfig . Writes rebuild backlinks and reindex the new entry. mx replace / patch / append (update entry) stateDiagram-v2 [*] --&gt; ResolvePath ResolvePath --&gt; ParseEntry ParseEntry --&gt; ApplyEdits ApplyEdits --&gt; WriteEntry WriteEntry --&gt; ReindexEntry ReindexEntry --&gt; Done Done --&gt; [*] Notes: Paths accept @project/ and @user/ prefixes. Edits update frontmatter metadata and reindex the entry. Search + Neighbors (Relations Graph) stateDiagram-v2 [*] --&gt; ValidateKB ValidateKB --&gt; RunSearch RunSearch --&gt; ApplyMinScore ApplyMinScore --&gt; IncludeNeighbors: --include-neighbors ApplyMinScore --&gt; FormatOutput: no neighbors IncludeNeighbors --&gt; ExpandFromSemanticLinks IncludeNeighbors --&gt; ExpandFromTypedRelations IncludeNeighbors --&gt; ExpandFromWikilinks ExpandFromSemanticLinks --&gt; MergeResults ExpandFromTypedRelations --&gt; MergeResults ExpandFromWikilinks --&gt; MergeResults MergeResults --&gt; FormatOutput FormatOutput --&gt; [*] Notes: --scope limits the KB roots used for indexing and search; single-KB mode returns unscoped paths. Neighbor expansion uses semantic links, typed relations, and wikilinks. Typed Relations + Publish Rendering mx relations-add / relations-remove stateDiagram-v2 [*] --&gt; ResolveEntryPath ResolveEntryPath --&gt; ParseEntry ParseEntry --&gt; UpdateRelations UpdateRelations --&gt; WriteFrontmatter WriteFrontmatter --&gt; ReindexEntry ReindexEntry --&gt; Done Done --&gt; [*] mx publish stateDiagram-v2 [*] --&gt; ResolveKBSource ResolveKBSource --&gt; UseKbRoot: --kb-root ResolveKBSource --&gt; UseScope: --scope ResolveKBSource --&gt; UseContext: .kbconfig/.kbconfig project_kb or kb_path ResolveKBSource --&gt; ErrorNoKB: none UseKbRoot --&gt; LoadEntries UseScope --&gt; LoadEntries UseContext --&gt; LoadEntries LoadEntries --&gt; BuildIndices BuildIndices --&gt; RenderPages RenderPages --&gt; BuildGraph BuildGraph --&gt; WriteSearchIndex WriteSearchIndex --&gt; Done Done --&gt; [*] Notes: Typed relation targets are normalized per-scope; cross-scope targets are skipped during publish. Publish outputs HTML pages, tag indexes, search index, and graph data."
    },
    {
      "id": "design/relations-graph/wikilink-edge-behavior",
      "title": "Wikilink Edge Behavior",
      "tags": "memex relations graph",
      "content": "Wikilink Edge Behavior Wikilinks are untyped edges captured from content. Related: design/relations-graph/relations-graph-overview ."
    },
    {
      "id": "design/relations-graph/relations-graph-overview",
      "title": "Relations Graph Overview",
      "tags": "memex relations graph",
      "content": "Relations Graph Overview This page connects wikilinks, typed relations, and semantic links into a single graph. See design/relations-graph/wikilink-edge-behavior and design/relations-graph/frontmatter-edge-types for details. Published UI Published KB pages surface typed relations in two places: Entry panel : &quot;Typed Relations&quot; shows outgoing vs incoming edges with direction arrows and type labels. Graph view : Typed relations render as solid edges with arrowheads; semantic links and wikilinks appear with distinct styles, and the controls let you filter by origin and relation type. Search neighbors mx search --include-neighbors expands results using semantic links, typed relations, and wikilinks. Use --neighbor-depth to control hop count (default 1). Query the relations graph There isn't a dedicated CLI command yet. For now: Use mx search --include-neighbors to expand results through semantic links + typed relations + wikilinks Inspect relations in entry frontmatter (open the file or use mx get path/to/entry.md --json ) Editing typed relations Edit the relations frontmatter directly (or via mx patch / mx replace ). relations: - path: reference/cli.md type: documents - path: ref/other.md type: implements"
    },
    {
      "id": "design/relations-graph/frontmatter-edge-types",
      "title": "Frontmatter Edge Types",
      "tags": "memex relations graph",
      "content": "Frontmatter Edge Types Frontmatter relations create typed edges with explicit types. Related: design/relations-graph/relations-graph-overview ."
    }
  ],
  "metadata": {
    "reference/focusgroup-evaluation-2026-01": {
      "title": "Focusgroup Agent UX Evaluation (2026-01)",
      "tags": [
        "focusgroup",
        "agent-ux",
        "evaluation",
        "cli"
      ],
      "path": "reference/focusgroup-evaluation-2026-01.html"
    },
    "reference/entry-format": {
      "title": "Entry Format Reference",
      "tags": [
        "reference",
        "frontmatter",
        "markdown",
        "links"
      ],
      "path": "reference/entry-format.html"
    },
    "reference/cli": {
      "title": "CLI Reference",
      "tags": [
        "cli",
        "reference",
        "commands"
      ],
      "path": "reference/cli.html"
    },
    "reference/chat-gpt-5.2-pro-dense-memory-signals-reference": {
      "title": "ChatGPT 5.2 Pro Dense Memory Signals Reference",
      "tags": [
        "reference",
        "ai",
        "memory",
        "signals"
      ],
      "path": "reference/chat-gpt-5.2-pro-dense-memory-signals-reference.html"
    },
    "reference/focusgroup-evaluation-mx-cli-discoverability-2026-01": {
      "title": "Focusgroup Evaluation: mx CLI Discoverability (2026-01)",
      "tags": [
        "focusgroup",
        "evaluation",
        "mx",
        "cli"
      ],
      "path": "reference/focusgroup-evaluation-mx-cli-discoverability-2026-01.html"
    },
    "reference/dense-memory-signals-research": {
      "title": "Dense Memory Signals Research",
      "tags": [
        "agent-memory",
        "research",
        "memory",
        "signals"
      ],
      "path": "reference/dense-memory-signals-research.html"
    },
    "guides/mcp-setup": {
      "title": "MCP Server Setup (Deprecated)",
      "tags": [
        "mcp",
        "claude-desktop",
        "setup",
        "configuration",
        "deprecated"
      ],
      "path": "guides/mcp-setup.html"
    },
    "guides/ai-integration": {
      "title": "AI Agent Integration",
      "tags": [
        "ai",
        "claude-code",
        "codex",
        "agents",
        "integration"
      ],
      "path": "guides/ai-integration.html"
    },
    "guides/quick-start": {
      "title": "Quick Start Guide",
      "tags": [
        "getting-started",
        "tutorial",
        "basics"
      ],
      "path": "guides/quick-start.html"
    },
    "guides/installation": {
      "title": "Installation Guide",
      "tags": [
        "installation",
        "setup",
        "getting-started"
      ],
      "path": "guides/installation.html"
    },
    "design/chunking-system-design": {
      "title": "Chunking System Design",
      "tags": [
        "memex",
        "architecture",
        "chunking",
        "a-mem"
      ],
      "path": "design/chunking-system-design.html"
    },
    "design/llm-query-processing-feature-plan": {
      "title": "LLM Query Processing Feature Plan",
      "tags": [
        "memex",
        "llm",
        "feature-plan"
      ],
      "path": "design/llm-query-processing-feature-plan.html"
    },
    "design/state-diagrams": {
      "title": "MX State Diagrams",
      "tags": [
        "memex",
        "architecture",
        "flows",
        "state-diagrams"
      ],
      "path": "design/state-diagrams.html"
    },
    "design/relations-graph/wikilink-edge-behavior": {
      "title": "Wikilink Edge Behavior",
      "tags": [
        "memex",
        "relations",
        "graph"
      ],
      "path": "design/relations-graph/wikilink-edge-behavior.html"
    },
    "design/relations-graph/relations-graph-overview": {
      "title": "Relations Graph Overview",
      "tags": [
        "memex",
        "relations",
        "graph"
      ],
      "path": "design/relations-graph/relations-graph-overview.html"
    },
    "design/relations-graph/frontmatter-edge-types": {
      "title": "Frontmatter Edge Types",
      "tags": [
        "memex",
        "relations",
        "graph"
      ],
      "path": "design/relations-graph/frontmatter-edge-types.html"
    }
  }
}